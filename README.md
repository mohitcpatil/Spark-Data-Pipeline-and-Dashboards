# Spark-Data-Pipeline-and-Dashboards

    This project workspace gives a rudimentary view of how ETL pipeline gets build, transform and loaded in Spark Dataframes (temporary view) which acts as table/dataset and those dataset are used for further Analytics and Reporting. There are multiple approaches to build a pipeline and make the data ready for analysis. While making a decision I took all factors are taken into consideration to mention a few. data integrations, scalability, cost-efficiency, batch or stream processing, usabability.

Summary of Jupyter notebook:
###### 1. Intorduction
- ETL Architecture
- Schema for Dataset 
###### 2. Data Extraction
- Spark Partition
###### 3. Data Transformation
- Spark API's
- Schema for Dataset
###### 4. Data Loading 
- Star Schema
###### 5. Analytics and Dashboards
- User Behavior Analytics
- Campaign Analytics
- Product Analytics
