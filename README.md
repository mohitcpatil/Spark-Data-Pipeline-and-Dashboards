# Spark-Data-Pipeline-and-Dashboards

   This project workspace gives a rudimentary view of how ETL pipeline gets build, transform and loaded in Spark Dataframes (temporary view) which acts as table/dataset and those dataset are used for further Analytics and Reporting. There are multiple approaches to build a pipeline and make the data ready for analysis. While making a decision I took all factors are taken into consideration to mention a few. data integrations, scalability, cost-efficiency, batch or stream processing, usabability.
   
<img width="1086" alt="ETL" src="https://user-images.githubusercontent.com/900824/114146170-22d3fa80-98cc-11eb-99eb-1b1474b425a3.png">

### Summary of Jupyter notebook:
###### 1. Intorduction
- ETL Architecture
- Schema for Dataset 
###### 2. Data Extraction
- Spark Partition
###### 3. Data Transformation
- Spark API's
- Schema for Dataset
###### 4. Data Loading 
- Star Schema
###### 5. Analytics and Dashboards
- User Behavior Analytics
- Campaign Analytics
- Product Analytics
